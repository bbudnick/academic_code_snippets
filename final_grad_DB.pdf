Willis H*** and Brita Hill
CS586: Databases Fall 2019
Graduate Project
Final Write-up

Hammer Time:
An Assessment of the Impact of RAM on DBMS Performance in HammerDB for OLTP and OLAP Benchmarks on PostgreSQL


INTRODUCTION

This project involved using the HammerDB open source benchmarking suite to run a battery of TPC-C and TPC-H tests against a PostgreSQL database hosted on Google Cloud. 

TPC-C:
The TPC-C benchmark simulates an OLTP (Online Transaction Processing) workload, similar to the workloads encountered by e-commerce platforms, banking systems, and ticket booking websites. In these systems, consistency of data over the duration of a transaction is of paramount importance. According to our research, “TPC-C stresses the entire stack (database system, operating system, and hardware) in several ways…. The database grows throughout the benchmarking.”1 

TPC-H:
The TPC-H benchmark simulates an OLAP (Online Analytical Processing) workload. This test is comprised of 22 queries, some short-running, others long-running. Our research revealed that “TPC-H observes more complicated long-running read-only queries with frequent index and file scans.”1 

SYSTEM CONFIGURATION

During our initial research phase, we discovered that Google Cloud offers products designed specifically for databases including fully managed relational database services. However, we elected to proceed according to the HammerDB recommendations which involved installing PostgreSQL from source and manually editing configuration files to ensure optimum performance. We installed PostgreSQL on a dedicated virtual machine. Here we describe some of the steps required to set up our system.


Figure 1: PostgreSQL was compiled from source on the nailman VM instance. 


Figure 2: HammerDB CLI verifying PostgreSQL library successfully loaded

TEST CONFIGURATION — TPC-C

We configured the TPC-C benchmark timed test with 1,000,000 transactions, 5 minutes of ramp-up time, and a 10-minute test duration. We ran this test for 1, 10, 20, 30, 40, and 50 virtual users. 



Figure 3: TPC-C schema build options

TEST CONFIGURATION — TPC-H

We configured our TPC-H schema using a scale factor of 1 and ran with 1, 10, 20, 30, and 40 virtual users. We attempted to use larger scale factors, but the time and space required to create these larger schemas made such tests impractical on a smaller system such as ours. 



Figure 4: TPC-H schema build

Properly configuring the TPC-H benchmark with PostgreSQL presented some major challenges. In the course of our research, we discovered that queries 17 and 20 tend to be long-running under PostgreSQL, and an option to disable these queries is included in the test driver script. During our initial tests, we observed similar behavior under query 21. In order to find what may have caused this behavior, we examined the HammerDB source code. In the query generation script we found the following three queries:
Query 17: 
SELECT SUM(l_extendedprice) / 7.0 AS avg_yearly 
FROM lineitem, part 
WHERE p_partkey = l_partkey 
	AND p_brand = ':1' 
	AND p_container = ':2' 
	AND l_quantity <
		(SELECT 0.2 * AVG(l_quantity) 
		FROM lineitem 
		WHERE l_partkey = p_partkey)
Query 20: 
SELECT s_name, s_address 
FROM supplier, nation 
WHERE s_suppkey in 
	(SELECT ps_suppkey 
	FROM partsupp 
	WHERE  ps_partkey in 
		(SELECT p_partkey 
		FROM part 
		WHERE p_name like ':1%') AND ps_availqty > 
			(SELECT 0.5 * SUM(l_quantity) 
			FROM lineitem 
			WHERE l_partkey = ps_partkey 
				AND l_suppkey = ps_suppkey 
				AND l_shipdate >= date ':2' 
				AND l_shipdate < date ':2' + interval '1 year')) 
			AND s_nationkey = n_nationkey 
			AND n_name = ':3' 
		ORDER BY s_name
Query 21: 
SELECT s_name, count(*) AS numwait 
FROM supplier, lineitem l1, orders, nation 
WHERE s_suppkey = l1.l_suppkey 
	AND o_orderkey = l1.l_orderkey 
	AND o_orderstatus = 'F' 
	AND l1.l_receiptdate > l1.l_commitdate 
	AND EXISTS 
		(SELECT * 
		FROM lineitem l2 
		WHERE l2.l_orderkey = l1.l_orderkey 
		AND l2.l_suppkey <> l1.l_suppkey) 
		AND NOT EXISTS
			 (SELECT * 
			FROM lineitem l3 
			WHERE l3.l_orderkey = l1.l_orderkey 
				AND l3.l_suppkey <> l1.l_suppkey 
				AND l3.l_receiptdate > l3.l_commitdate) 
		AND s_nationkey = n_nationkey 
		AND n_name = ':1' 
GROUP BY s_name
ORDER BY numwait desc, s_name
Surprisingly, there didn’t seem to be anything unusual about these queries compared to the others. However, in the interest of time, we decided to run all tests with them disabled. The original driver script contained an option to disable long-running queries, and we modified the driver to disable query 21 as well.

Figure 5: Modified driver script to skip query 21





RESULTS & ANALYSIS — TPC-C: Initial VM Configuration

Our first battery of TPC-C tests ran on our standard test machine with 4 virtual cores and 7.5GB RAM. The transaction counter utility displayed some erratic behavior during these tests regardless of the amount of ramp-up time. 


Figure 6: Initial virtual machine configuration


Figure 7: Transaction counter showing TPM for 10 virtual users



Figure 8: TPC-C test result for 10 virtual users


Figure 9: CPU utilization during single-user test

Figure 10: TPM vs. number of virtual users

The number of transactions per minute diminished in a roughly linear fashion as the number of virtual users increased. This was relatively unsurprising, because we expect that our system, having limited resources, would perform worse as it encountered more simultaneous transaction requests.

RESULTS & ANALYSIS — TPC-C: Second VM Configuration

In order to test the effects of hardware changes on our load server, we decided to reconfigure a new virtual machine with identical specifications to our first VM, but equipped with 26GB RAM. We reasoned that since CPU loads were relatively low (usually around 35%) during TPC-C benchmarking, either RAM or SSD was acting as a bottleneck. 


Figure 11: Second configuration of PostgreSQL DBMS VM (nailman)



Figure 12: Transaction counter showing steady state performance with 10 virtual users


Figure 13: Test results with 10 virtual users




In addition to the tests displayed above, we conducted tests 20, 30, 40, and 50 virtual users and compiled the data into a spreadsheet. Below is a chart showing the impact of the number of virtual users on overall system performance:

Figure 14: TPM vs. number of virtual users

The results of adding additional RAM were surprising. This additional memory capacity significantly reduced the level of variation in transactions per minute and made the attainment of a steady state possible. Most importantly, the number of transactions per minute roughly doubled. This strongly implied that memory is the major bottleneck for TPC-C tests on entry-level cloud machines. We feel confident that the HammerDB benchmarking suite has significant problems working with machines with a lower memory footprint.

RESULTS & ANALYSIS — TPC-H
Our TPC-H tests were run with the initial VM configuration described above, with 4 virtual cores and 7.5GB RAM. We ran tests for 1, 10, 20, 30, and 40 virtual users.

Figure 15: OLAP test with single user


Figure 16: Time to completion per query for TPC-H with a single user. Note that queries 17, 20, and 21 were skipped and therefore have no time to completion.
During our test with a single virtual user, we observed that queries took between 0.24s and 8.061s to run to completion. The geometric mean of the time per query was 1.047s, while the average was 1.577s. Since there we didn’t run this test on different hardware configurations, we were unable to assess the impact of additional RAM on TPC-H performance.

Figure 17: Test results under TPC-H with 10 virtual users


Figure 18: Time to completion for 20 virtual users on TPC-H



Figure 19: Time to completion for 30 virtual users on TPC-H 


Figure 20: Time to completion for 40 virtual users under TPC-H


Total virtual users
Mean time to completion for 22 queries per virtual user 
10
24.15
20
23.45
30
24.28
40
24.56

Figure 21: TPC-H mean time to completion per virtual user for our 22 queries vs. number of virtual users


As seen above, performance did not measurably change as the number of virtual users scaled up. This could be due to the fact that PostgreSQL is not well optimized for concurrent processing. “PostgreSQL does not support support parallel query and therefore the workload profile is usually (depending on I/O) a single core per query running at 100% CPU with other cores standing idle.”2

CONCLUSION

Our results were largely consistent with our expectations, but we noticed some idiosyncrasies that might merit further investigation. In particular, running benchmarks on a smaller system presented some challenges. We suspect that we never actually achieved a steady state during TPC-C testing because the transactions per user tended fluctuate by as much as 50% during each test. Regardless, we did see results that confirmed our expectations about memory being a bottleneck during heavy transactional workloads. Performance increased significantly when the amount of available RAM was increased from 7.5GB to 26GB.

Future research might make use of more capable hardware and increase the number of tests to generate more consistent results and achieve greater statistical power when performing analysis of the results. 

In the process of researching how to establish and test a database, we wondered how much difference machine configuration could be testably proven to make. We wondered about the importance of third-party testing to ensure that metrics are standardized and not skewed to favor a company’s particular configuration which might not honestly demonstrate their products’ capacity.
Works Cited

1. https://openproceedings.org/2013/conf/edbt/TozunPKJA13.pdf
2. https://sourceforge.net/p/hammerdb/discussion/292313/thread/1044916f/
